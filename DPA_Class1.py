import os
import re
import sys
import time
import glob
import logging
import platform
import warnings
import argparse
import subprocess
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import rpy2.robjects as robjects
import rpy2.robjects.pandas2ri as pandas2ri
from time import time
from typing import List
from pathlib import Path
from datetime import datetime
from matplotlib import gridspec
from dataclasses import dataclass
from collections import defaultdict
from rpy2.robjects import r, IntVector
from rpy2.robjects.packages import importr
from upsetplot import from_memberships, UpSet
from typing import Tuple, Optional, List, Dict
from rpy2.robjects import pandas2ri, ListVector, StrVector
from rpy2.rinterface_lib.callbacks import logger as rpy2_logger
from concurrent.futures import ProcessPoolExecutor, as_completed

# Initialize R interface
pandas2ri.activate()

# Import required R packages
base = importr('base')
grid = importr('grid')
readr = importr('readr')
pvclust = importr('pvclust')
venn = importr('VennDiagram')
rankprod = importr('RankProd')


# Suppress warnings and set logging level
warnings.filterwarnings('ignore', category=FutureWarning)
rpy2_logger.setLevel(logging.ERROR)
logging.basicConfig(level=logging.INFO, format='%(message)s')
pd.set_option('future.no_silent_downcasting', True)

@dataclass
class Config:
    model_path: str
    problems_dir_fba: str
    problems_dir_ko: str
    template_fba: str
    template_ko: str
    output_dir_fba: str
    output_dir_ko: str
    fba_executable: str
    expression_dir: str
    output_dir: str
    base_dir: str  # Add base directory
    producibility_results_dir: str  # Add specific results directory
    rankprod_output_dir: str  # Add RankProd output directory
    top_ranks_dir: str  # Add Top_Ranks directory
    timeout_seconds: int = 90

######################################################################################################################################################################################
# Functions from the first script

def display_help():
    """Display help information when --help flag is used"""
    help_text = """
╔════════════════════════════════════════════════════════════════════════════════════════╗
║  DPA (Differentail Producibility Analysis Tool): A tool for Metabolic Network Analysis ║
╚════════════════════════════════════════════════════════════════════════════════════════╝

Usage:
  python DPA_tool1.py [OPTIONS]  # For one-class data without replicates
  python DPA_tool2.py [OPTIONS]  # For two-class analysis with replicates

Options:
  --help      Display this help message

Description:
  This tool analyzes metabolic networks using Flux Balance Analysis (FBA) 
  and Knockout (KO) studies to measure metabolite producibility.
  
  • DPA_tool1.py - One-class analysis for unpaired data
  • DPA_tool2.py - Two-class analysis requiring biological replicates
                  (Use _Rep1, _Rep2, etc. naming for replicates)

Prerequisites:
  • Expression data folder (CSV files with gene_id column and log2FoldChange column)
  • GSMN-TBupdated_original.sfba (Metabolic Network Model)
  • Problem_template_FBA.pfile
  • sfba-glpk executable (GLPK solver)
  • Conda environment from DPA.yaml file

Workflow:
  1. FBA-KO Analysis → Calculate metabolite producibility
  2. RankProd Analysis → Rank metabolites by expression
  3. Visualization → Generate diagrams and plots

Interactive Prompts:
  • Expression mapping: 'higher' or 'lower' priority
  • Threshold for up/down regulation
  • Condition combinations for analysis
  • Statistical thresholds for selecting top ranking metabolites based on 
    p-value, PFP and ranks(generated by rank product analysis)

For complete documentation, please see README.md

"""
    print(help_text)

def clean_directories_if_needed(config: Config) -> None:
    """
    Check if any of the specified directories contain files. If any do, prompt the user once to remove files from all directories.
    """
    directories_to_clean = [
        config.problems_dir_fba,
        config.problems_dir_ko,
        config.output_dir_fba,
        config.output_dir_ko,
        config.producibility_results_dir,
        config.rankprod_output_dir,
        config.output_dir,
        config.top_ranks_dir,
        os.path.join(config.top_ranks_dir, "Downregulated"),
        os.path.join(config.top_ranks_dir, "Upregulated"),
        os.path.join(config.base_dir, "Infographics", "Downregulated"),
        os.path.join(config.base_dir, "Infographics", "Upregulated")
    ]

    # Check if any directory contains files
    has_files = False
    for directory in directories_to_clean:
        if os.path.exists(directory) and any(os.scandir(directory)):
            has_files = True
            break

    if has_files:
        print("\nSome directories contain files from previous runs.")
        user_input = input("Do you want to remove all files from these directories? (yes/no): ").strip().lower()
        if user_input == 'yes':
            for directory in directories_to_clean:
                if os.path.exists(directory) and any(os.scandir(directory)):
                    for file in os.listdir(directory):
                        file_path = os.path.join(directory, file)
                        if os.path.isfile(file_path):
                            os.remove(file_path)
                    
        else:
            print()
            
# Add the generate_ko_file function to your script
def generate_ko_file(fba_content):
    # Split the content into lines and clean them
    lines = [line.strip() for line in fba_content.strip().split('\n') if line.strip()]

    # Extract reaction data
    reaction_data = []
    reaction_ids = []

    for line in lines:
        # Skip header lines or empty lines
        if line.startswith('!') or line.startswith(';') or not line:
            continue

        # Extract reaction ID and data
        parts = line.split(None, 3)  # Split by whitespace for max 3 splits
        if len(parts) >= 3:
            reaction_id = parts[0]
            min_val = parts[1]
            max_val = parts[2]
            comment = parts[3] if len(parts) > 3 else ""

            reaction_data.append((reaction_id, min_val, max_val, comment))
            reaction_ids.append(reaction_id)

    # Build the KO file content
    ko_content = "!max: \n"

    # Create the header line with reaction IDs
    header_line = "? " + " ".join(reaction_ids)
    ko_content += header_line + "\n"

    # Add all reaction data
    for reaction in reaction_data:
        reaction_id, min_val, max_val, comment = reaction
        ko_content += f"{reaction_id}\t{min_val}\t{max_val}\t{comment}\n"

    # Add terminating semicolon
    ko_content += ";"

    return ko_content

def setup_directories(config: Config) -> None:
    for directory in [config.problems_dir_fba, config.problems_dir_ko, 
                     config.output_dir_fba, config.output_dir_ko,
                     config.output_dir]:
        os.makedirs(directory, exist_ok=True)

def extract_metabolites(model_path: str, config: Config) -> List[str]:
    """
    Extract and log metabolites from model file, excluding numeric values
    """
    metabolites = set()
    with open(model_path, 'r') as f:
        for line in f:
            columns = line.strip().split('\t')
            if len(columns) > 1:
                reaction_eq = re.sub(r'#.*$', '', columns[1])
                reaction_eq = re.sub(r'[+=<>]', ' ', reaction_eq)
                
                # Filter out any purely numeric values (integers or floats)
                for potential_metabolite in reaction_eq.split():
                    # Skip empty strings
                    if not potential_metabolite:
                        continue
                    # Skip if the string represents a float/numeric value
                    if not re.match(r'^[-+]?[0-9]*\.?[0-9]+$', potential_metabolite):
                        metabolites.add(potential_metabolite)
    
    sorted_metabolites = sorted(list(metabolites))
    log_metabolites(sorted_metabolites, config)
    return sorted_metabolites


def log_metabolites(metabolites: List[str], config: Config) -> None:
    """
    Log the validated metabolites to a file
    """
    log_path = os.path.join(config.base_dir, "metabolites_log.txt")
    
    with open(log_path, 'w') as f:
        f.write("Network Metabolites List\n")
        f.write("=" * 50 + "\n\n")
        f.write(f"Total number of metabolites: {len(metabolites)}\n\n")
        f.write("Metabolites:\n")
        for idx, metabolite in enumerate(metabolites, 1):
            f.write(f"{idx}. {metabolite}\n")
    
    print(f"Metabolites log file created at: {log_path}")

def run_analysis(metabolite: str, analysis_type: str, config: Config) -> str:
    problems_dir = config.problems_dir_fba if analysis_type == "FBA" else config.problems_dir_ko
    output_dir = config.output_dir_fba if analysis_type == "FBA" else config.output_dir_ko
    template_file = config.template_fba if analysis_type == "FBA" else config.template_ko
    
    sanitized_metabolite = metabolite.replace(":", "-")
    problem_file = os.path.join(problems_dir, f"Problem_{analysis_type}_{sanitized_metabolite}.pfile")
    output_file = os.path.join(output_dir, f"Metabolite_{sanitized_metabolite}.{analysis_type.lower()}out")

    with open(template_file, 'r') as template:
        content = template.read().replace("!max: ", f"!max: {metabolite}")
    
    with open(problem_file, 'w') as f:
        f.write(content)

    cmd = [config.fba_executable, "-i", config.model_path, "-p", analysis_type.lower(),
           "-b", problem_file, "-f", output_file, "-X", "_xt", "-o", metabolite,
           "-s", "simplex", "-c"]

    try:
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=config.timeout_seconds)
        if result.returncode == 0:
            if analysis_type == "KO":
                check_zero_output(output_file, sanitized_metabolite, output_dir)
            return f"{analysis_type} analysis completed for {metabolite}"
        return f"{analysis_type} analysis failed for {metabolite}: {result.stderr}"
    except subprocess.TimeoutExpired:
        for file in [problem_file, output_file]:
            if os.path.exists(file):
                os.remove(file)
        return f"Timeout: {metabolite} took longer than {config.timeout_seconds} seconds"
    except Exception as e:
        return f"Error in {analysis_type} analysis for {metabolite}: {str(e)}"

def check_zero_output(output_file: str, metabolite: str, output_dir: str) -> None:
    try:
        with open(output_file, 'r') as f:
            if all(line.strip().split()[1] == '0:' for line in f if len(line.strip().split()) > 1):
                undefined_file = os.path.join(output_dir, f"Undefined_{metabolite}.koout")
                os.rename(output_file, undefined_file)
                print(f"All zero values detected for {metabolite}")
    except Exception as e:
        print(f"Error checking zero output for {metabolite}: {str(e)}")

def cleanup_empty_files(config: Config) -> None:
    """Clean up empty files from output directories."""
    directories = [config.output_dir_fba, config.output_dir_ko]
    for directory in directories:
        for filename in os.listdir(directory):
            filepath = os.path.join(directory, filename)
            if os.path.getsize(filepath) == 0:
                os.remove(filepath)
                print(f"Removed empty file: {filepath}")

def cleanup_mismatched_files(config: Config) -> None:
    """Clean up mismatched files from output directories."""
    fba_files = set(f.replace('.fbaout', '') for f in os.listdir(config.output_dir_fba) 
                    if f.endswith('.fbaout'))
    ko_files = set(f.replace('.koout', '') for f in os.listdir(config.output_dir_ko) 
                   if f.endswith('.koout'))
    
    # Remove mismatched FBA files
    mismatched = fba_files - ko_files
    for metabolite in mismatched:
        fba_file = os.path.join(config.output_dir_fba, f"{metabolite}.fbaout")
        if os.path.exists(fba_file):
            os.remove(fba_file)
            print(f"Removed mismatched FBA file: {fba_file}")

    # Remove mismatched KO files
    mismatched = ko_files - fba_files
    for metabolite in mismatched:
        ko_file = os.path.join(config.output_dir_ko, f"{metabolite}.koout")
        if os.path.exists(ko_file):
            os.remove(ko_file)
            print(f"Removed mismatched KO file: {ko_file}")

def clean_problem_files(base_dir, directories):
    """
    Remove problem files with numeric-only names.
    """
    for dir_name in directories:
        full_path = os.path.join(base_dir, dir_name)
        
        # Check if directory exists
        if not os.path.exists(full_path):
            print(f"Directory not found: {full_path}")
            continue
        
        # Iterate through files in the directory
        for filename in os.listdir(full_path):
            # Check if file matches the problem file naming pattern
            if filename.startswith(('Problem_FBA_', 'Problem_KO_')) and filename.endswith('.pfile'):
                # Remove file if it contains only numbers and decimal
                if re.match(r'^Problem_(?:FBA|KO)_[0-9.]+\.pfile$', filename):
                    file_path = os.path.join(full_path, filename)
                    os.remove(file_path)
                    print(f"Removed: {file_path}")
            
warnings.filterwarnings("ignore", category=DeprecationWarning)

class MetaboliteAnalyzer:
    def __init__(self, base_dir: str, output_dir: str, prioritization: str = 'higher', regulation_threshold: float = 2):
        """
        Initialize the MetaboliteAnalyzer and expression file handling.
        """
        self.prioritization = prioritization
        self.regulation_threshold = regulation_threshold
        # Validate base directory
        if not os.path.exists(base_dir):
            raise FileNotFoundError(f"Base directory does not exist: {base_dir}")

        self.base_dir = base_dir
        self.output_dir = output_dir
        self.fba_dir = os.path.join(base_dir, "FBA_result")
        self.ko_dir = os.path.join(base_dir, "KO_result")
        self.expression_dir = os.path.join(base_dir, "Expression")
        # Find the first .sfba file in the base directory
        sfba_files = list(Path(base_dir).glob("*.sfba"))

        if not sfba_files:
            raise FileNotFoundError("No .sfba files found in the base directory.")

        self.gsmn_file = sfba_files[0]  # Store the first matching file

        print("GSMN file set to:", self.gsmn_file)
        
        self._validate_directories()
        self.expression_files = self._get_expression_files()
        self.conditions = list(self.expression_files.keys())
        self.producibility_threshold = 0.001
        os.makedirs(self.output_dir, exist_ok=True)

    def _get_expression_files(self) -> Dict[str, str]:
        """
        Discover and map expression files in the Expression directory.
        """
        expression_files = {}
        
        if not os.path.exists(self.expression_dir):
            raise FileNotFoundError(f"Expression directory not found: {self.expression_dir}")
            
        for file_name in os.listdir(self.expression_dir):
            if file_name.endswith('.csv'):
                # Extract condition name from file name (remove file extension)
                condition = os.path.splitext(file_name)[0]
                # Create full file path
                file_path = os.path.join(self.expression_dir, file_name)
                expression_files[condition] = file_path
                
        if not expression_files:
            raise FileNotFoundError(f"No CSV files found in Expression directory: {self.expression_dir}")
            
        return expression_files

    def _validate_directories(self):
        """Validate the existence of required directories and files."""
        required_items = {
            "FBA results directory": self.fba_dir,
            "KO results directory": self.ko_dir,
            "GSMN file": self.gsmn_file,
            "Expression directory": self.expression_dir
        }
        
        missing_items = {name: path for name, path in required_items.items() 
                        if not os.path.exists(path)}
        
        if missing_items:
            error_msg = "The following required items are missing:\n"
            error_msg += "\n".join(f"- {name}: {path}" 
                                 for name, path in missing_items.items())
            raise FileNotFoundError(error_msg)

    def calculate_log2fc(self, rule: str, condition: str, 
                        expression_data: Dict[str, pd.DataFrame], 
                        prioritization: str = 'higher') -> Optional[float]:
        """
        Calculate log2 fold change for a given rule and condition.
        """
        try:
            if not rule or pd.isna(rule):
                return None

            exp_data = expression_data.get(condition)
            if exp_data is None:
                return None

            # Convert rule to uppercase for consistent processing
            rule = rule.upper()
            
            # First split by OR to get major groups
            or_groups = [group.strip() for group in rule.split(' OR ')]
            group_values = []

            for group in or_groups:
                if ' AND ' in group:
                    # Handle AND operations within each OR group
                    genes = [gene.strip() for gene in group.split(' AND ')]
                    gene_values = []
                    
                    for gene in genes:
                        # Clean up gene ID and check if it exists in the data
                        gene = gene.strip('()')
                        if gene in exp_data.index:
                            gene_values.append(exp_data.loc[gene, 'log2FoldChange'])
                    
                    # For AND operations, use the mean of the values
                    if gene_values:
                        group_values.append(sum(gene_values) / len(gene_values))
                else:
                    # Handle single genes
                    gene = group.strip('()')
                    if gene in exp_data.index:
                        group_values.append(exp_data.loc[gene, 'log2FoldChange'])
            
            if not group_values:
                return None
                
            # For OR operations, use max or min based on prioritization
            return max(group_values) if prioritization == 'higher' else min(group_values)

        except Exception as e:
            print(f"Error calculating log2fc for rule {rule} and condition {condition}: {e}")
            return None

    def load_gsmn_data(self) -> pd.DataFrame:
        """Load GSMN data with error handling."""
        try:
            column_names = ['ID', 'equation', 'LB', 'UB', 'Rule', 'hash', 'comment']
            gsmn_data = pd.read_csv(self.gsmn_file, sep="\t", header=None)
            
            if gsmn_data.empty:
                raise ValueError(f"GSMN file is empty: {self.gsmn_file}")
                
            gsmn_data.columns = column_names[:len(gsmn_data.columns)]
            gsmn_data['Rule'] = gsmn_data['Rule'].str.split('#').str[0].str.strip()
            
            return gsmn_data
            
        except Exception as e:
            raise ValueError(f"Error loading GSMN file: {str(e)}")

    def load_expression_data(self) -> Dict[str, pd.DataFrame]:
        """Load all expression data files with improved gene ID handling."""
        expression_data = {}
        
        for condition, file_path in self.expression_files.items():
            try:
                exp_data = pd.read_csv(file_path)
                
                if exp_data.empty:
                    raise ValueError(f"Expression file is empty: {file_path}")
                
                required_columns = ['gene_id', 'log2FoldChange']
                missing_columns = [col for col in required_columns 
                                 if col not in exp_data.columns]
                
                if missing_columns:
                    raise ValueError(
                        f"Expression file {file_path} missing required columns: "
                        f"{', '.join(missing_columns)}"
                    )
                
                # Convert gene IDs to uppercase for consistent matching
                exp_data['gene_id'] = exp_data['gene_id'].str.upper()
                exp_data.set_index('gene_id', inplace=True)
                
                # Add debug information
                print(f"Loaded {len(exp_data)} genes from {condition}")
                print(f"Sample gene IDs: {list(exp_data.index[:5])}")
                
                expression_data[condition] = exp_data
                
            except Exception as e:
                raise ValueError(f"Error loading expression file {file_path}: {str(e)}")
        
        return expression_data
    
    def parse_flux_file(self, file_path: str, is_ko: bool = False) -> Dict[str, float]:
        """Parse flux files with error handling."""
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"Flux file not found: {file_path}")
            
        try:
            fluxes = {}
            with open(file_path, 'r') as file:
                content = file.read().strip()
                if not content:
                    raise ValueError(f"Flux file is empty: {file_path}")
                    
                for line in content.split('\n'):
                    if line.startswith('#') or not line.strip():
                        continue
                    parts = line.split()
                    if len(parts) < 2:
                        continue
                    reaction_id = parts[0]
                    try:
                        flux_value = float(parts[1]) if not is_ko else float(parts[1].split(':')[0])
                        fluxes[reaction_id] = flux_value
                    except ValueError:
                        print(f"Warning: Invalid flux value in file {file_path} "
                              f"for reaction {reaction_id}")
                        continue
            
            if not fluxes:
                raise ValueError(f"No valid flux values found in file: {file_path}")
                
            return fluxes
            
        except Exception as e:
            raise ValueError(f"Error parsing flux file {file_path}: {str(e)}")

    def calculate_producibility(self, fba_fluxes: Dict[str, float], 
                              ko_fluxes: Dict[str, float]) -> pd.DataFrame:
        """Calculate producibility differences between FBA and KO fluxes."""
        data = {
            "Reaction_FBA": [],
            "Reaction_KO": [],
            "Producibility_Difference": [],
            "Affects_Producibility": []
        }
        
        all_reactions = set(fba_fluxes.keys()).union(set(ko_fluxes.keys()))
        for reaction in all_reactions:
            wild_flux = fba_fluxes.get(reaction, 0)
            mutant_flux = ko_fluxes.get(reaction, 0)
            df_mg = mutant_flux - wild_flux
            
            data["Reaction_FBA"].append(reaction)
            data["Reaction_KO"].append(reaction)
            data["Producibility_Difference"].append(df_mg)
            data["Affects_Producibility"].append(
                1 if abs(df_mg) > self.producibility_threshold else 0
            )
        
        return pd.DataFrame(data)

    def analyze_metabolites(self):
        """
        Analyze metabolites with improved gene rule handling.
        """
        try:
            # Load data
            gsmn_data = self.load_gsmn_data()
            expression_data = self.load_expression_data()
                
            # Clean and preprocess rules
            gsmn_data = gsmn_data[gsmn_data['Rule'].notna()]
            gsmn_data['Rule'] = gsmn_data['Rule'].str.strip()
            gsmn_data = gsmn_data[gsmn_data['Rule'] != '']
                
            # Add log2FoldChange columns for each condition
            for condition in self.conditions:
                column_name = f"log2FoldChange_{condition}"
                gsmn_data[column_name] = gsmn_data['Rule'].apply(
                    lambda rule: self.calculate_log2fc(
                        rule, condition, expression_data, self.prioritization
                    )
                )
            
            # Process each FBA file
            for fba_file in os.listdir(self.fba_dir):
                if fba_file.endswith('.fbaout'):
                    self._process_metabolite_file(fba_file, gsmn_data)

        except Exception as e:
            print(f"Error in analyze_metabolites: {str(e)}")
            raise

    def _process_metabolite_file(self, fba_file: str, gsmn_data: pd.DataFrame):
        """Process individual metabolite files with improved rule handling."""
        fba_file_path = os.path.join(self.fba_dir, fba_file)
        ko_file_name = fba_file.replace('.fbaout', '.koout')
        ko_file_path = os.path.join(self.ko_dir, ko_file_name)
        
        try:
            metabolite_name = fba_file.split('Metabolite_')[-1].replace('.fbaout', '')
        except Exception as e:
            print(f"Could not extract metabolite name from {fba_file}: {e}")
            metabolite_name = "Unknown"
        
        if os.path.exists(ko_file_path):
            try:
                fba_fluxes = self.parse_flux_file(fba_file_path)
                ko_fluxes = self.parse_flux_file(ko_file_path, is_ko=True)
                
                producibility_data = self.calculate_producibility(fba_fluxes, ko_fluxes)
                
                # Only filter after merging to preserve all relevant reactions
                merged_data = producibility_data.merge(
                    gsmn_data,
                    left_on="Reaction_FBA",
                    right_on="ID",
                    how="inner"
                )
                
                # Apply producibility filter after merging
                merged_data = merged_data[
                    merged_data["Affects_Producibility"] == 1
                ]
                
                output_df = self._prepare_final_dataframe(merged_data, metabolite_name)
                
                csv_file_path = os.path.join(
                    self.output_dir, 
                    f"{fba_file.replace('.fbaout', '')}_final_metabolite_analysis.csv"
                )
                
                # Add debug information
                print(f"Processing {metabolite_name}:")
                print(f"Original reactions: {len(producibility_data)}")
                print(f"After merging: {len(merged_data)}")
                print(f"Final output rows: {len(output_df)}")
                
                output_df.to_csv(csv_file_path, index=False)
                print(f"Saved analysis for {metabolite_name} to {csv_file_path}")
                
            except Exception as e:
                print(f"Error processing {fba_file}: {str(e)}")
        else:
            print(f"KO file not found for {fba_file}")

    def _prepare_final_dataframe(self, merged_data: pd.DataFrame, 
                               metabolite_name: str) -> pd.DataFrame:
        """Prepare the final output dataframe with improved signal calculations."""
        # Initialize base columns
        final_data = {
            "Reaction_name": merged_data["Reaction_FBA"],
            "Producibility_Difference": merged_data["Producibility_Difference"],
            "Affects_Producibility": merged_data["Affects_Producibility"],
            "equation": merged_data["equation"],
            "Rule": merged_data["Rule"],
            "Metabolite": [metabolite_name] * len(merged_data)
        }
        
        # Add log2FoldChange columns and calculate signals for each condition
        for condition in self.conditions:
            final_data[f"log2FoldChange_{condition}"] = merged_data[
                f"log2FoldChange_{condition}"
            ]
            
            # Calculate signal values with user-defined threshold
            condition_data = merged_data[f"log2FoldChange_{condition}"]
            up_signals = condition_data[condition_data > self.regulation_threshold]
            down_signals = condition_data[condition_data < self.regulation_threshold]
            
            up_median = up_signals.median() if not up_signals.empty else 0
            down_median = down_signals.median() if not down_signals.empty else 0
            
            final_data[f"Upregulated_Signal_{condition}"] = [up_median] * len(merged_data)
            final_data[f"Downregulated_Signal_{condition}"] = [down_median] * len(merged_data)
        
        return pd.DataFrame(final_data)


def get_regulation_thresholds():
    """
    Get user-defined thresholds for upregulated and downregulated values.
    Returns the threshold value where:
    - Values > threshold are considered upregulated
    - Values < threshold are considered downregulated
    
    Default is 0 if no input is provided.
        """
    try:
        threshold = input("Enter threshold for up/down regulation (default is 0, press Enter to use default): ").strip()
        if not threshold:
            return 0  # Default value
        
        threshold_value = float(threshold)
        print(f"Using regulation threshold: {threshold_value}")
        print(f"Values > {threshold_value} will be considered upregulated")
        print(f"Values < {threshold_value} will be considered downregulated")
        return threshold_value
    except ValueError:
        print("Invalid input. Using default threshold of 0.")
        return 0


class RankProdGenerator:
    def __init__(self, input_dir: str, output_dir: str):
        """
        Initialize RankProd matrix generator.
        """
        self.input_dir = input_dir
        self.output_dir = output_dir
        
        # Validate directories
        if not os.path.exists(input_dir):
            raise FileNotFoundError(f"Input directory not found: {input_dir}")
        
        os.makedirs(output_dir, exist_ok=True)

    def _get_signal_columns(self, df: pd.DataFrame) -> Tuple[List[str], List[str]]:
        """
        Identify upregulated and downregulated signal columns.
        """
        up_cols = [col for col in df.columns if col.startswith('Upregulated_Signal_')]
        down_cols = [col for col in df.columns if col.startswith('Downregulated_Signal_')]
        
        if not up_cols or not down_cols:
            raise ValueError("No signal columns found in the data")
            
        return up_cols, down_cols

    def _extract_condition_names(self, columns: List[str]) -> List[str]:
        """
        Extract condition names from column headers.
        """
        # Extract condition names by removing the prefix
        conditions = [col.replace('Upregulated_Signal_', '').replace('Downregulated_Signal_', '') 
                     for col in columns if col.startswith(('Upregulated_Signal_', 'Downregulated_Signal_'))]
        
        # Get unique conditions
        return list(set(conditions))

    def generate_matrices(self) -> Tuple[Optional[pd.DataFrame], Optional[pd.DataFrame]]:
        """
        Generate upregulated and downregulated matrices from metabolite analysis files.
        """
        # Find all CSV files
        csv_files = [f for f in os.listdir(self.input_dir) if f.endswith('_final_metabolite_analysis.csv')]
        
        if not csv_files:
            print(f"No CSV files found in: {self.input_dir}")
            return None, None

        first_file = True
        upregulated_signals = []
        downregulated_signals = []
        conditions = []

        for csv_file in csv_files:
            file_path = os.path.join(self.input_dir, csv_file)
            print(f"Processing: {csv_file}")

            try:
                df = pd.read_csv(file_path)
                
                if df.empty:
                    print(f"Empty file: {csv_file}")
                    continue

                # Get signal columns from first file
                if first_file:
                    up_cols, down_cols = self._get_signal_columns(df)
                    conditions = self._extract_condition_names(up_cols + down_cols)
                    first_file = False

                # Validate required columns
                required_cols = ['Metabolite'] + up_cols + down_cols
                if not all(col in df.columns for col in required_cols):
                    missing_cols = [col for col in required_cols if col not in df.columns]
                    print(f"Missing columns in {csv_file}: {missing_cols}")
                    continue

                # Extract metabolite name
                metabolite = df['Metabolite'].iloc[0]

                # Prepare upregulated signals
                up_row = {'Metabolite': metabolite}
                for condition in conditions:
                    col_name = f'Upregulated_Signal_{condition}'
                    up_row[condition] = df[col_name].iloc[0]
                upregulated_signals.append(up_row)

                # Prepare downregulated signals
                down_row = {'Metabolite': metabolite}
                for condition in conditions:
                    col_name = f'Downregulated_Signal_{condition}'
                    down_row[condition] = df[col_name].iloc[0]
                downregulated_signals.append(down_row)

                print(f"Processed metabolite: {metabolite}")

            except Exception as e:
                print(f"Error processing {csv_file}: {str(e)}")
                continue

        if not upregulated_signals or not downregulated_signals:
            print("No data processed successfully")
            return None, None

        # Convert to DataFrames
        up_matrix = pd.DataFrame(upregulated_signals)
        down_matrix = pd.DataFrame(downregulated_signals)

        # Set index
        up_matrix.set_index('Metabolite', inplace=True)
        down_matrix.set_index('Metabolite', inplace=True)

        return up_matrix, down_matrix

    def save_matrices(self, up_matrix: Optional[pd.DataFrame], 
                     down_matrix: Optional[pd.DataFrame]) -> None:
        """
        Save the generated matrices to CSV files.
        """
        if up_matrix is None or down_matrix is None:
            print("No matrices to save")
            return

        try:
            # Save upregulated matrix
            up_path = os.path.join(self.output_dir, 'upregulated_matrix.csv')
            up_matrix.to_csv(up_path)
            print(f"Saved upregulated matrix to: {up_path}")

            # Save downregulated matrix
            down_path = os.path.join(self.output_dir, 'downregulated_matrix.csv')
            down_matrix.to_csv(down_path)
            print(f"Saved downregulated matrix to: {down_path}")

        except Exception as e:
            print(f"Error saving matrices: {str(e)}")

# Modify the pipeline_analysis_main function
def pipeline_analysis_main(config: Config):
    # First set up directories
    setup_directories(config)
    
    # Extract metabolites and create log
    print("Extracting and logging metabolites from network...")
    metabolites = extract_metabolites(config.model_path, config)
    print(f"Found {len(metabolites)} metabolites in the network")
    
    # Generate the KO file from the FBA template
    print("Generating KO file from FBA template...")
    with open(config.template_fba, 'r') as f:
        fba_content = f.read()
    
    ko_content = generate_ko_file(fba_content)
    
    with open(config.template_ko, 'w') as f:
        f.write(ko_content)
    
    print("KO file generated successfully!")
    
    # Run parallel analysis
    with ProcessPoolExecutor() as executor:
        print("Starting FBA analysis...")
        future_to_metabolite = {
            executor.submit(run_analysis, m, "FBA", config): m 
            for m in metabolites
        }
        for future in as_completed(future_to_metabolite):
            print(future.result())

        print("\nStarting KO analysis...")
        future_to_metabolite = {
            executor.submit(run_analysis, m, "KO", config): m 
            for m in metabolites
        }
        for future in as_completed(future_to_metabolite):
            print(future.result())

    cleanup_mismatched_files(config)
    cleanup_empty_files(config)
    print("Pipeline analysis finished!")

def metabolite_analysis_main(config: Config):
    analyzer = MetaboliteAnalyzer(config.base_dir, config.producibility_results_dir)
    analyzer.analyze_metabolites()

def rankprod_generation_main(config: Config):
    try:
        generator = RankProdGenerator(
            config.producibility_results_dir, 
            config.rankprod_output_dir
        )
        up_matrix, down_matrix = generator.generate_matrices()
        generator.save_matrices(up_matrix, down_matrix)
    except Exception as e:
        print(f"Error in RankProd generation: {str(e)}")

######################################################################################################################################################################################
# Functions from the second script

def process_results(r_result, matrix_data, filename, condition1, condition2, is_upregulated):
    # Extract relevant components from the R result
    rps = np.array(r_result.rx2('RPs'))  # Rank Products
    rprank = np.array(r_result.rx2('RPrank'))  # Rank Product ranks
    pval = np.array(r_result.rx2('pval'))  # p-values
    pfp = np.array(r_result.rx2('pfp'))  # Percentage of false positives
    ave_fc = np.array(r_result.rx2('AveFC'))  # Average fold change

    # Subset columns for the specific conditions
    cond1_cols = [col for col in matrix_data.columns if condition1 in col]
    cond2_cols = [col for col in matrix_data.columns if condition2 in col]
    
    # Print column details
    print(f"\nInput Expression Columns:")
    print(f"{condition1} columns: {cond1_cols}")
    print(f"{condition2} columns: {cond2_cols}")
    
    # Take the last column of each condition as the expression value
    cond1_exp = matrix_data[cond1_cols.pop()].to_numpy()  # Take the last column
    cond2_exp = matrix_data[cond2_cols.pop()].to_numpy()  # Take the last column

    # Create a DataFrame with the desired columns
    results_df = pd.DataFrame({
        "Metabolite": matrix_data.index,  # Add Metabolite names as the first column
        f"{condition1}_Exp": cond1_exp,  # Last column expression for condition1
        f"{condition2}_Exp": cond2_exp,  # Last column expression for condition2
        f"{condition2} \\ {condition1}": rps[:, 0],  # Cond2 \ Cond1
        f"{condition1} / {condition2}": rps[:, 1],  # Cond1 / Cond2
        f"{condition1} < {condition2} rank": rprank[:, 0],  # Cond1 < Cond2 rank
        f"{condition1} > {condition2} rank": rprank[:, 1],  # Cond1 > Cond2 rank
        f"{condition1} < {condition2} p.value": pval[:, 0],  # Cond1 < Cond2 p.value
        f"{condition1} > {condition2} p. value": pval[:, 1],  # Cond1 > Cond2 p. value
        f"{condition1} < {condition2} pfp value": pfp[:, 0],  # Cond1 < Cond2 pfp value
        f"{condition1} > {condition2} pfp value": pfp[:, 1],  # Cond1 > Cond2 pfp value
        "Log(2) Average Cond1/Cond2": ave_fc.flatten()  # Log(2) Average Cond1/Cond2
    })

    # Sort by p-value (Cond1 > Cond2 p.value)
    results_df = results_df.sort_values(f"{condition1} > {condition2} rank")

    # Create Results directory if it doesn't exist
    if not os.path.exists("Results"):
        os.makedirs("Results")

    # Modify filename based on data type
    if is_upregulated:
        filename = f"upregulated_{filename}"
    else:
        filename = f"downregulated_{filename}"

    # Save the results to a CSV file
    output_path = os.path.join("Results", filename)
    results_df.to_csv(output_path, index=False, float_format="%.9f")
    print(f"Results saved to {output_path}")
    return results_df

def read_and_prepare_matrix(file_path):
    matrix_data = pd.read_csv(file_path, index_col=0)
    matrix_data = matrix_data.fillna(0)
    return matrix_data

def get_condition_names(matrix_data):
    col_names = matrix_data.columns
    condition_names = pd.Series(col_names).str.replace('_Rep[0-9]+$', '', regex=True).unique()
    return list(condition_names)

def get_all_permutations(conditions):
    from itertools import permutations
    perm_list = list(permutations(conditions, 2))
    
    print("\nAvailable condition permutations:")
    for i, (cond1, cond2) in enumerate(perm_list, 1):
        print(f"{i}: {cond1} vs {cond2}")
    
    return perm_list

def get_user_selected_permutations(permutations):
    """
    Prompt the user to select condition permutations for analysis.
    
    Args:
        permutations (list): List of tuples containing condition permutations.
    
    Returns:
        list: List of tuples containing user-selected condition permutations.
    """
    print("\nAvailable condition permutations:")
    for i, (cond1, cond2) in enumerate(permutations, 1):
        print(f"{i}: {cond1} vs {cond2}")
    
    while True:
        try:
            user_input = input("\nEnter the numbers of the permutations you want to analyze (comma-separated), or press Enter to select all: ").strip()
            
            if not user_input:
                print("All permutations selected.")
                return permutations
            
            selected_indices = list(map(int, user_input.split(',')))
            
            if any(idx < 1 or idx > len(permutations) for idx in selected_indices):
                raise ValueError("Invalid combination number(s). Please try again.")
            
            selected_permutations = [permutations[idx - 1] for idx in selected_indices]
            return selected_permutations
        except ValueError as e:
            print(f"Error: {e}")

def perform_analysis_on_selected_permutations(up_matrix, down_matrix, selected_permutations):
    """
    Perform RankProd analysis on the user-selected condition permutations.
    
    Args:
        up_matrix (pd.DataFrame): Upregulated matrix.
        down_matrix (pd.DataFrame): Downregulated matrix.
        selected_permutations (list): List of tuples containing user-selected condition permutations.
    """
    total_permutations = len(selected_permutations)
    print(f"\nPerforming analysis for {total_permutations} selected permutations...")

    for cond1, cond2 in selected_permutations:
        print(f"\nAnalyzing {cond1} vs {cond2} for Upregulated...")
        perform_analysis(up_matrix, cond1, cond2, is_upregulated=True)

        print(f"\nAnalyzing {cond1} vs {cond2} for Downregulated...")
        perform_analysis(down_matrix, cond1, cond2, is_upregulated=False)

def perform_analysis(matrix_data, cond1, cond2, is_upregulated):
    """
    Perform one-class analysis (unpaired, when no replicates are available)
    Class 1 uses cl=1 for all samples
    """
    # Subset the matrix for the two conditions
    cond1_cols = [col for col in matrix_data.columns if cond1 in col]
    cond2_cols = [col for col in matrix_data.columns if cond2 in col]
    subset_matrix = matrix_data[cond1_cols + cond2_cols]
    
    # Print analysis info
    print(f"\nPerforming Class 1 Analysis (one-class/unpaired)")
    print(f"Total samples: {subset_matrix.shape[1]}")
    print(f"{cond1} samples: {len(cond1_cols)}")
    print(f"{cond2} samples: {len(cond2_cols)}")
    
    # Convert to R matrix
    r_matrix = pandas2ri.py2rpy(subset_matrix)
    
    # Create class labels for one-class analysis (all samples are class 1)
    cl = robjects.IntVector([1] * subset_matrix.shape[1])
    
    print(f"Class vector: {list(cl)}")
    
    # Perform RankProd analysis
    result = rankprod.RankProducts(
        data=r_matrix,
        cl=cl,
        logged=True,
        rand=1000,
        na_rm=True,
        gene_names=robjects.StrVector(list(subset_matrix.index))
    )
    
    # Process results 
    filename = f"{cond1}_vs_{cond2}_results.csv"
    
    if is_upregulated:
        results = process_results(result, subset_matrix, filename, cond1, cond2, is_upregulated=True)
    else:
        results = process_results(result, subset_matrix, filename, cond1, cond2, is_upregulated=False)
    
    return results

## Extract TOP_RANKS ##

def get_user_threshold_preferences():
    print("\nMetabolite Selection Settings:")
    
    # Ask for selection method
    print("Select metabolites based on:")
    print("1. Rank (default)")
    print("2. p-value")
    print("3. PFP value")
    
    selection_choice = input("Enter your choice (1-3) or press Enter for default: ").strip()
    
    if not selection_choice or selection_choice == '1':
        selection_method = 'rank'
        top_n = int(input("Enter number of top metabolites to extract (default: 100): ") or 100)
        return {
            'selection_method': selection_method,
            'top_n': top_n,
            'threshold_value': None
        }
    
    elif selection_choice == '2':
        selection_method = 'pvalue'
        threshold = float(input("Enter p-value threshold (e.g., 0.05): ") or 0.05)
        top_n = int(input("Enter maximum number of metabolites to extract: ") or 100)
        return {
            'selection_method': selection_method,
            'top_n': top_n,
            'threshold_value': threshold
        }
    
    elif selection_choice == '3':
        selection_method = 'pfp'
        threshold = float(input("Enter PFP threshold (e.g., 0.05): ") or 0.05)
        top_n = int(input("Enter maximum number of metabolites to extract: ") or 100)
        return {
            'selection_method': selection_method,
            'top_n': top_n,
            'threshold_value': threshold
        }
    
    else:
        print("Invalid choice. Using default selection by rank.")
        return {
            'selection_method': 'rank',
            'top_n': 100,
            'threshold_value': None
        }

def extract_top_metabolites():
    # Get user preferences for selection method
    selection_params = get_user_threshold_preferences()
    
    # Create Top_Ranks directory and subdirectories
    main_dir = "Top_Ranks"
    up_dir = os.path.join(main_dir, "Upregulated")
    down_dir = os.path.join(main_dir, "Downregulated")
    
    for directory in [main_dir, up_dir, down_dir]:
        if not os.path.exists(directory):
            os.makedirs(directory)
            print(f"Created directory: {directory}")
    
    # Get all RankProd result files
    results_path = "Results"
    up_files = glob.glob(os.path.join(results_path, "upregulated_*.csv"))
    down_files = glob.glob(os.path.join(results_path, "downregulated_*.csv"))
    
    print(f"Found {len(up_files)} upregulated files and {len(down_files)} downregulated files")
    
    # Process both upregulated and downregulated files
    process_files(up_files, up_dir, selection_params, is_upregulated=True)
    process_files(down_files, down_dir, selection_params, is_upregulated=False)
    
    # Print summary of what was done
    method_str = selection_params['selection_method']
    if method_str == 'rank':
        print(f"Processing complete. Top {selection_params['top_n']} ranked metabolites extracted to Top_Ranks directory.")
    elif method_str == 'pvalue':
        print(f"Processing complete. Up to {selection_params['top_n']} metabolites with p-value ≥ {selection_params['threshold_value']} extracted to Top_Ranks directory.")
    elif method_str == 'pfp':
        print(f"Processing complete. Up to {selection_params['top_n']} metabolites with PFP ≥ {selection_params['threshold_value']} extracted to Top_Ranks directory.")

def process_files(file_list, output_dir, selection_params, is_upregulated):
    for file_path in file_list:
        filename = os.path.basename(file_path)
        parts = filename.split('_')
        condition1 = parts[1]
        condition2 = parts[3]
        
        try:
            df = pd.read_csv(file_path)
            
            # First, get all columns that might contain relevant information
            rank_columns = [col for col in df.columns if 'rank' in col.lower() and '>' in col]
            pvalue_columns = [col for col in df.columns if 'p.value' in col.lower() and '>' in col]
            pfp_columns = [col for col in df.columns if 'pfp value' in col.lower() and '>' in col]
            
            # Check if we have columns to process
            if not rank_columns:
                print(f"Warning: No rank columns found for condition1 > condition2 in {filename}")
                continue
                
            # Process each rank column independently
            for rank_col in rank_columns:
                # Confirm this is a column where condition1 > condition2
                if not (condition1 in rank_col and '>' in rank_col):
                    continue
                
                # Direction string for output
                direction = f"{condition1}_{'upregulated' if is_upregulated else 'downregulated'}_vs_{condition2}"
                
                # Find corresponding p-value and pfp columns
                matching_pvalue_col = None
                matching_pfp_col = None
                
                # Look for matching p-value column
                for p_col in pvalue_columns:
                    if (condition1 in p_col and condition2 in p_col and '>' in p_col):
                        matching_pvalue_col = p_col
                        break
                
                # Look for matching PFP column
                for pfp_col in pfp_columns:
                    if (condition1 in pfp_col and condition2 in pfp_col and '>' in pfp_col):
                        matching_pfp_col = pfp_col
                        break
                
                # Select metabolites based on user's chosen method
                method = selection_params['selection_method']
                top_n = selection_params['top_n']
                threshold = selection_params['threshold_value']
                
                # Create a copy of the DataFrame to avoid modifying the original
                working_df = df.copy()
                
                if method == 'rank':
                    # Sort by rank and take top N
                    selected_df = working_df.sort_values(by=rank_col).head(top_n)
                    output_filename = f"top{top_n}_by_rank_{direction}.csv"
                
                elif method == 'pvalue' and matching_pvalue_col:
                    # Filter by p-value and take up to top N
                    filtered_df = working_df[working_df[matching_pvalue_col] >= threshold]  # ≥ threshold
                    # Sort by p-value (ascending order starting from threshold)
                    selected_df = filtered_df.sort_values(by=matching_pvalue_col).head(top_n)
                    output_filename = f"top{top_n}_pvalue_from_{threshold}_{direction}.csv"
                
                elif method == 'pfp' and matching_pfp_col:
                    # Filter by PFP and take up to top N
                    filtered_df = working_df[working_df[matching_pfp_col] >= threshold]  # ≥ threshold
                    # Sort by PFP (ascending order starting from threshold)
                    selected_df = filtered_df.sort_values(by=matching_pfp_col).head(top_n)
                    output_filename = f"top{top_n}_pfp_from_{threshold}_{direction}.csv"
                
                else:
                    # If the method doesn't match available columns, fall back to rank
                    selected_df = working_df.sort_values(by=rank_col).head(top_n)
                    output_filename = f"top{top_n}_by_rank_{direction}.csv"
                    if method != 'rank':
                        print(f"Warning: Could not find matching columns for {method} in {filename}, falling back to rank")
                
                # Create output with all relevant columns
                if len(selected_df) > 0:
                    # Include important columns in the output
                    output_cols = ["Metabolite", rank_col]
                    
                    # Add p-value and pfp columns if they exist
                    if matching_pvalue_col:
                        output_cols.append(matching_pvalue_col)
                    if matching_pfp_col:
                        output_cols.append(matching_pfp_col)
                    
                    # Add expression columns if they exist
                    exp_cols = [col for col in df.columns if '_Exp' in col]
                    output_cols.extend(exp_cols)
                    
                    # Create the output DataFrame with only existing columns
                    output_df = selected_df[[col for col in output_cols if col in selected_df.columns]]
                    
                    # Save to file
                    output_path = os.path.join(output_dir, output_filename)
                    output_df.to_csv(output_path, index=False)
                    print(f"Saved {len(output_df)} metabolites to {output_filename}")
                else:
                    print(f"Warning: No metabolites matched criteria for {direction}")
                    # Create empty file with headers to maintain consistency
                    output_cols = ["Metabolite", rank_col]
                    if matching_pvalue_col:
                        output_cols.append(matching_pvalue_col)
                    if matching_pfp_col:
                        output_cols.append(matching_pfp_col)
                    pd.DataFrame(columns=output_cols).to_csv(
                        os.path.join(output_dir, output_filename), index=False
                    )
            
            print(f"Successfully processed {filename}")
            
        except Exception as e:
            print(f"Error processing file {filename}: {str(e)}")
            import traceback
            traceback.print_exc()
            
######################################################################################################################################################################################
# Functions from the third script            

def load_metabolites(file_path):
    """
    Load metabolites from a file containing already extracted top metabolites.
    
    Args:
        file_path (str): Path to the CSV file
    
    Returns:
        list: List of metabolite names
    """
    try:
        # Load the CSV file
        df = pd.read_csv(file_path)
        
        # First try common column names for metabolites
        possible_column_names = ['Metabolite', 'metabolite', 'Name', 'name', 'Compound', 'compound', 'ID', 'id']
        
        for col_name in possible_column_names:
            if col_name in df.columns:
                metabolites = df[col_name].tolist()
                return [m for m in metabolites if m and not pd.isna(m)]  # Filter out None and NaN values
        
        # If no matching column found, use the first column
        metabolites = df.iloc[:, 0].tolist()
        return [m for m in metabolites if m and not pd.isna(m)]  # Filter out None and NaN values
    
    except Exception as e:
        print(f"Error loading file {file_path}: {e}")
        return []

def parse_filename(filename):
    """
    Parse condition and comparison from filename using multiple patterns.
    
    Args:
        filename (str): Filename to parse
    
    Returns:
        tuple: (condition, comparison)
    """
    basename = os.path.basename(filename)
    
    # Pattern 1: top{N}_{Condition}_{regulation}_vs_{Comparison}.csv
    pattern1 = r"top\d+_([^_]+)_(?:up|down)regulated_vs_([^.]+)\.csv"
    match = re.search(pattern1, basename)
    if match:
        return match.group(1), match.group(2)
    
    # Pattern 2: {Condition}_vs_{Comparison}_{regulation}.csv
    pattern2 = r"([^_]+)_vs_([^_]+)_(?:up|down)regulated\.csv"
    match = re.search(pattern2, basename)
    if match:
        return match.group(1), match.group(2)
    
    # Pattern 3: {Condition}_{regulation}_vs_{Comparison}.csv
    pattern3 = r"([^_]+)_(?:up|down)regulated_vs_([^.]+)\.csv"
    match = re.search(pattern3, basename)
    if match:
        return match.group(1), match.group(2)
    
    # Try to extract just by looking for "vs" as a fallback
    parts = basename.split('_')
    if 'vs' in parts:
        vs_index = parts.index('vs')
        if vs_index > 0 and vs_index + 1 < len(parts):
            # Try to identify the condition and comparison
            # Assume condition is before "vs" and comparison after "vs"
            condition_candidates = [p for p in parts[:vs_index] if not p.isdigit() and p.lower() not in ["top", "upregulated", "downregulated"]]
            comparison_candidates = [p.split('.')[0] for p in parts[vs_index+1:]]
            
            if condition_candidates and comparison_candidates:
                return condition_candidates[0], comparison_candidates[0]
    
    print(f"Could not parse filename: {basename}, using basename as condition")
    # As a last resort, use the basename without extension as the condition
    condition = os.path.splitext(basename)[0]
    return condition, "unknown"

def export_type1_csv(metabolites_by_condition_comparison, output_path, condition, regulation_type):
    """
    Export Type 1 (Individual Comparisons) data to CSV.
    
    Args:
        metabolites_by_condition_comparison (dict): Dictionary with condition_comparison keys and metabolite lists
        output_path (str): Path to save the CSV
        condition (str): Condition name
        regulation_type (str): 'upregulated' or 'downregulated'
    """
    # Flatten the data structure to get all unique metabolites
    all_metabolites = set()
    for metabolites in metabolites_by_condition_comparison.values():
        all_metabolites.update(metabolites)
    
    # Create a dictionary to track which comparisons each metabolite appears in
    metabolite_data = {}
    for metabolite in all_metabolites:
        appearances = []
        for condition_comparison, metabolites in metabolites_by_condition_comparison.items():
            if metabolite in metabolites:
                appearances.append(condition_comparison)
        
        metabolite_data[metabolite] = {
            'Metabolite': metabolite,
            'Number_of_Comparisons': len(appearances),
            'Comparisons': ','.join(appearances)
        }
    
    # Convert to DataFrame and save as CSV
    df = pd.DataFrame(list(metabolite_data.values()))
    if not df.empty:
        df = df.sort_values(by=['Number_of_Comparisons', 'Metabolite'], ascending=[False, True])
        
        csv_path = os.path.join(output_path, f"{condition.lower()}__{regulation_type.lower()}_venn_data.csv")
        df.to_csv(csv_path, index=False)
        print(f"Type 1 CSV saved to {csv_path}")
    else:
        print(f"No data to export for Type 1 {regulation_type} CSV for condition {condition}")

def export_Global_Overlap_csv(all_condition_metabolites, output_path, regulation_type):
    """
    Export Type 2 (All Conditions) data to CSV.
    
    Args:
        all_condition_metabolites (dict): Dictionary with condition keys and metabolite lists
        output_path (str): Path to save the CSV
        regulation_type (str): 'upregulated' or 'downregulated'
    """
    # Flatten the data structure to get all unique metabolites
    all_metabolites = set()
    for metabolites in all_condition_metabolites.values():
        all_metabolites.update(metabolites)
    
    # Create a dictionary to track which conditions each metabolite appears in
    metabolite_data = {}
    for metabolite in all_metabolites:
        appearances = []
        for condition, metabolites in all_condition_metabolites.items():
            if metabolite in metabolites:
                appearances.append(condition)
        
        metabolite_data[metabolite] = {
            'Metabolite': metabolite,
            'Number_of_Conditions': len(appearances),
            'Conditions': ','.join(appearances)
        }
    
    # Convert to DataFrame and save as CSV
    df = pd.DataFrame(list(metabolite_data.values()))
    if not df.empty:
        df = df.sort_values(by=['Number_of_Conditions', 'Metabolite'], ascending=[False, True])
        
        csv_path = os.path.join(output_path, f"Global_Overlap_{regulation_type.lower()}_venn_data.csv")
        df.to_csv(csv_path, index=False)
        print(f"Type 2 CSV saved to {csv_path}")
    else:
        print(f"No data to export for Type 2 {regulation_type} CSV")

def create_venn_diagram(data_dict, output_file, custom_colors=None):
    """
    Create a Venn diagram using R's VennDiagram package.
    
    Args:
        data_dict (dict): Dictionary with set names as keys and lists of elements as values
        output_file (str): Path to save the output file
        custom_colors (list, optional): List of colors to use for the diagram
    
    Returns:
        bool: True if successful, False otherwise
    """
    if len(data_dict) < 2:
        print(f"Cannot create Venn diagram with less than 2 sets (found {len(data_dict)})")
        return False
    
    # Set default colors if not provided
    if not custom_colors or len(custom_colors) < len(data_dict):
        default_colors = ["#1F77B4", "#FF7F0E", "#2CA02C", "#D62728", "#9467BD", "#8C564B", "#E377C2", "#7F7F7F"]
        colors = default_colors[:len(data_dict)]
    else:
        colors = custom_colors[:len(data_dict)]
    
    # Convert Python dict to R list
    r_data_dict = ListVector({
        k: StrVector(v) for k, v in data_dict.items()
    })
    
    try:
        # Create the directory if it doesn't exist
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        
        # Create Venn diagram using R
        venn.venn_diagram(
            x=r_data_dict,
            category_names=StrVector(data_dict.keys()),
            filename=output_file,
            output=True,
            fill=StrVector(colors),
            alpha=0.5,
            # Increased image size and text parameters
            resolution=300,  # Set resolution to 300 DPI
            imagetype="png",
            cat_default_pos="outer",  # Position labels outside circles
            cat_cex=4,  # Increase category label size
            cat_fontface="bold",  # Make category labels bold
            cat_fontfamily="sans",  # Use sans-serif font
            margin=0.15,  # Increase margin to prevent cutting off labels
            width=3000,  # Increase width in pixels
            height=3000  # Increase height in pixels
        )
        print(f"Venn diagram saved to {output_file}")
        return True
    except Exception as e:
        print(f"Error creating Venn diagram: {e}")
        return False

def create_condition_specific_venn_diagrams(regulation_type):
    """
    Create condition-specific Venn diagrams and export data to CSV.
    
    Args:
        regulation_type (str): 'upregulated' or 'downregulated'
    """
    # Define file paths
    base_path = os.getcwd()
    input_path = os.path.join(base_path, "Top_Ranks", regulation_type.capitalize())
    output_path = os.path.join(base_path, "Infographics", regulation_type.lower())
    
    print(f"Using input path: {input_path}")
    print(f"Using output path: {output_path}")
    
    # Create output directory
    os.makedirs(output_path, exist_ok=True)
    
    # Check if input directory exists
    if not os.path.exists(input_path):
        print(f"Input directory not found: {input_path}")
        return
    
    # Find all CSV files in the input directory and dynamically determine conditions
    file_list = []
    condition_set = set()
    
    for file in os.listdir(input_path):
        if file.endswith('.csv'):
            file_path = os.path.join(input_path, file)
            condition, comparison = parse_filename(file)
            
            if condition and comparison:
                condition_set.add(condition)
                file_list.append((file_path, condition, comparison))
    
    conditions = sorted(list(condition_set))
    
    # Group files by condition
    condition_files = {condition: [] for condition in conditions}
    
    for file_path, condition, comparison in file_list:
        condition_files[condition].append((file_path, comparison))
    
    # Print summary of found files
    print(f"\nFound files for {regulation_type}:")
    print(f"Detected conditions: {', '.join(conditions)}")
    
    # Process each condition
    for condition, files in condition_files.items():
        print(f"  {condition}: {len(files)} files")
        for file_path, comparison in files:
            print(f"    - Compared with {comparison}: {os.path.basename(file_path)}")
        
        metabolites_by_comparison = {}
        for file_path, comparison in files:
            metabolites = load_metabolites(file_path)
            if metabolites:
                comparison_key = f"{condition}_vs_{comparison}"
                metabolites_by_comparison[comparison] = metabolites
        
        if len(metabolites_by_comparison) < 2:
            print(f"Skipping Venn diagram for {condition} - insufficient comparisons")
            continue
        
        output_file = os.path.join(output_path, f"{condition.lower()}_venn.png")
        create_venn_diagram(metabolites_by_comparison, output_file)
        
        export_type1_csv(metabolites_by_comparison, output_path, condition, regulation_type)

def create_global_overlap_venn_diagram(regulation_type):
    """
    Create a global overlap Venn diagram for all conditions and export data to CSV.
    
    Args:
        regulation_type (str): 'upregulated' or 'downregulated'
    """
    # Define file paths
    base_path = os.getcwd()
    input_path = os.path.join(base_path, "Top_Ranks", regulation_type.capitalize())
    output_path = os.path.join(base_path, "Infographics", regulation_type.lower())
    
    print(f"Using input path: {input_path}")
    print(f"Using output path: {output_path}")
    
    # Create output directory
    os.makedirs(output_path, exist_ok=True)
    
    # Check if input directory exists
    if not os.path.exists(input_path):
        print(f"Input directory not found: {input_path}")
        return
    
    # Find all CSV files in the input directory and dynamically determine conditions
    file_list = []
    condition_set = set()
    
    for file in os.listdir(input_path):
        if file.endswith('.csv'):
            file_path = os.path.join(input_path, file)
            condition, comparison = parse_filename(file)
            
            if condition and comparison:
                condition_set.add(condition)
                file_list.append((file_path, condition, comparison))
    
    conditions = sorted(list(condition_set))
    
    # Group files by condition
    condition_files = {condition: [] for condition in conditions}
    
    for file_path, condition, comparison in file_list:
        condition_files[condition].append((file_path, comparison))
    
    # Collect metabolites for all conditions
    all_condition_metabolites = {}
    for condition in conditions:
        combined_metabolites = set()
        for file_path, comparison in condition_files[condition]:
            metabolites = load_metabolites(file_path)
            combined_metabolites.update(metabolites)
        
        if combined_metabolites:
            all_condition_metabolites[condition] = list(combined_metabolites)
    
    if len(all_condition_metabolites) < 2:
        print(f"Skipping global overlap Venn diagram - insufficient conditions with data")
        return
    
    output_file = os.path.join(output_path, "global_overlap_venn.png")
    create_venn_diagram(all_condition_metabolites, output_file)
    
    export_Global_Overlap_csv(all_condition_metabolites, output_path, regulation_type)

## UPSET-PLOT ##

def create_inverted_upset_plot(Global_Overlap_csv_path, output_file, regulation_type, custom_conditions=None, max_metabolites=None):
    """
    Create an inverted UpSet plot where metabolites are on the vertical axis and conditions are horizontal.
    Modified to handle 300+ metabolites with appropriate scaling.
    
    Args:
        Global_Overlap_csv_path (str): Path to the Type 2 CSV file
        output_file (str): Path to save the output plot
        regulation_type (str): 'upregulated' or 'downregulated'
        custom_conditions (list, optional): Custom ordering of conditions
        max_metabolites (int, optional): Maximum number of metabolites to include in the plot
    """
    try:
        # Load the Type 2 CSV data
        if not os.path.exists(Global_Overlap_csv_path):
            print(f"Error: File not found - {Global_Overlap_csv_path}")
            return False
            
        df = pd.read_csv(Global_Overlap_csv_path)
        
        if df.empty:
            print(f"Error: Empty CSV file - {Global_Overlap_csv_path}")
            return False
            
        # Check required columns
        required_columns = ['Metabolite', 'Conditions']
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            print(f"Error: Missing required columns in CSV file: {', '.join(missing_columns)}")
            return False
        
        # Get all unique conditions
        all_conditions = set()
        for conditions_str in df['Conditions']:
            if isinstance(conditions_str, str):  # Check if it's a string
                conditions = conditions_str.split(',')
                all_conditions.update(conditions)
        
        # If no conditions found or parsing issue
        if not all_conditions:
            print("Error: No valid conditions found in the CSV file")
            return False
        
        # Use custom conditions ordering if provided, otherwise sort alphabetically
        if custom_conditions:
            # Make sure all conditions from the data are included
            missing_from_custom = all_conditions - set(custom_conditions)
            all_conditions = custom_conditions + sorted(list(missing_from_custom))
        else:
            all_conditions = sorted(list(all_conditions))
        
        # Limit the number of metabolites if requested
        if max_metabolites and max_metabolites < len(df):
            print(f"Limiting plot to top {max_metabolites} metabolites by number of conditions")
            # Calculate number of conditions per metabolite
            condition_counts = df['Conditions'].apply(lambda x: len(x.split(',')) if isinstance(x, str) else 0)
            # Sort by condition count (descending) and take top N
            df = df.loc[condition_counts.sort_values(ascending=False).index[:max_metabolites]]
        
        # Get all metabolites
        all_metabolites = df['Metabolite'].tolist()
        num_metabolites = len(all_metabolites)
        
        print(f"Processing {num_metabolites} metabolites across {len(all_conditions)} conditions")
        
        # Calculate appropriate figure size based on number of metabolites
        # Use a base height per metabolite and adjust for readability
        height_per_metabolite = 0.25  # inches per metabolite
        min_height = 20  # minimum figure height
        max_height = 150  # maximum figure height to prevent excessive sizes
        
        figure_height = max(min_height, min(max_height, num_metabolites * height_per_metabolite))
        figure_width = 24  # Fixed width
        
        # Create binary matrix for set membership
        binary_matrix = pd.DataFrame(0, index=all_metabolites, columns=all_conditions)
        
        for idx, row in df.iterrows():
            metabolite = row['Metabolite']
            if isinstance(row['Conditions'], str):  # Check if it's a string
                conditions = row['Conditions'].split(',')
                for condition in conditions:
                    if condition in all_conditions:  # Make sure the condition exists in our list
                        binary_matrix.loc[metabolite, condition] = 1
        
        # Sort metabolites by the number of conditions they appear in
        metabolite_condition_counts = binary_matrix.sum(axis=1)
        sorted_metabolites = metabolite_condition_counts.sort_values(ascending=False).index.tolist()
        
        # Create the figure with custom size
        fig = plt.figure(figsize=(figure_width, figure_height))
        
        # Define the grid with appropriate ratios
        # Adjust the height ratios to give more space to the metabolite list
        gs = gridspec.GridSpec(3, 3, 
                              height_ratios=[0.1, 0.8, 0.1], 
                              width_ratios=[0.25, 0.7, 0.05])
        
        # Create axes
        metabolite_count_ax = plt.subplot(gs[1, 0])  # Left side - count of conditions per metabolite
        main_ax = plt.subplot(gs[1, 1])  # Main matrix
        condition_count_ax = plt.subplot(gs[0, 1])  # Top - count of metabolites per condition
        
        # Customizable colors
        main_color = 'lightblue'  # Blue for main elements
        highlight_color = 'navy'  # Navy for filled dots
        
        # Prepare data for plotting
        y_positions = np.arange(len(sorted_metabolites))
        x_positions = np.arange(len(all_conditions))
        
        # Plot metabolite condition counts (how many conditions each metabolite appears in)
        bar_height = 0.8  # Height of each bar
        for i, count in enumerate(metabolite_condition_counts[sorted_metabolites]):
            metabolite_count_ax.barh(y_positions[i], count, height=bar_height, color=main_color, edgecolor='black')
        
        # Adaptive font size calculation for metabolite labels
        # Use a formula that gradually reduces font size as the number of metabolites increases
        if num_metabolites <= 20:
            font_size = 14
        elif num_metabolites <= 50:
            font_size = 12
        elif num_metabolites <= 100:
            font_size = 10
        elif num_metabolites <= 200:
            font_size = 8
        else:
            font_size = 6
        
        metabolite_count_ax.set_yticks(y_positions)
        metabolite_count_ax.set_yticklabels(sorted_metabolites, fontsize=font_size)
        metabolite_count_ax.set_title('Metabolites', fontsize=20)
        
        # Set y-axis limits to match the main matrix
        metabolite_count_ax.set_ylim(-0.5, len(sorted_metabolites) - 0.5)
        
        # Plot condition metabolite counts (how many metabolites appear in each condition)
        condition_metabolite_counts = binary_matrix.sum(axis=0)
        condition_count_ax.bar(x_positions, condition_metabolite_counts, color=main_color)
        condition_count_ax.set_xticks(x_positions)
        condition_count_ax.set_xticklabels(all_conditions, rotation=90, fontsize=18)
        condition_count_ax.set_title('Conditions', fontsize=20)
        
        # Plot main matrix (presence/absence of metabolites in conditions)
        binary_matrix_sorted = binary_matrix.loc[sorted_metabolites]
        
        # Adaptive marker size based on number of metabolites
        marker_size = max(10, min(50, 5000 / num_metabolites))
        
        # Create the matrix plot (plot by condition for efficiency)
        for j, condition in enumerate(all_conditions):
            condition_data = binary_matrix_sorted[condition]
            present_indices = np.where(condition_data == 1)[0]
            if len(present_indices) > 0:
                main_ax.scatter([j] * len(present_indices), present_indices, 
                               color=highlight_color, s=marker_size, marker='o')
        
        # Connect dots horizontally for each metabolite (efficiency improvement)
        # For large datasets, plot connection lines in batches to prevent memory issues
        batch_size = 100  # Process this many metabolites at a time for large datasets
        
        if num_metabolites > 500:
            print("Large dataset detected. Processing connection lines in batches...")
            
        for batch_start in range(0, len(sorted_metabolites), batch_size):
            batch_end = min(batch_start + batch_size, len(sorted_metabolites))
            batch_metabolites = sorted_metabolites[batch_start:batch_end]
            
            for i, metabolite in enumerate(batch_metabolites, start=batch_start):
                condition_indices = np.where(binary_matrix_sorted.loc[metabolite] == 1)[0]
                if len(condition_indices) > 1:  # Only draw lines if there are at least 2 points
                    main_ax.plot(condition_indices, [i] * len(condition_indices), 
                                color=highlight_color, linewidth=1, alpha=0.5)
        
        # Add grid lines with adaptive approach
        # Adjust grid line density and opacity based on number of metabolites
        grid_alpha = max(0.05, min(0.2, 20 / num_metabolites))
        
        if num_metabolites <= 100:
            # For smaller datasets, draw all horizontal lines
            for i in range(len(sorted_metabolites)):
                main_ax.axhline(y=i, color='gray', linestyle='-', alpha=grid_alpha)
        else:
            # For larger datasets, draw fewer lines to avoid visual clutter
            step = max(1, len(sorted_metabolites) // 50)
            for i in range(0, len(sorted_metabolites), step):
                main_ax.axhline(y=i, color='gray', linestyle='-', alpha=grid_alpha)
        
        # Always draw vertical grid lines (usually fewer)
        for j in range(len(all_conditions)):
            main_ax.axvline(x=j, color='gray', linestyle='-', alpha=grid_alpha)
        
        # Set axis labels and ticks
        main_ax.set_yticks(y_positions)
        main_ax.set_yticklabels([])  # Hide y-tick labels as they're shown in metabolite_count_ax
        main_ax.set_xticks(x_positions)
        main_ax.set_xticklabels([])  # Hide x-tick labels as they're shown in condition_count_ax
        
        # Set limits for main plot
        main_ax.set_xlim(-0.5, len(all_conditions) - 0.5)
        main_ax.set_ylim(-0.5, len(sorted_metabolites) - 0.5)
        
        # Title with increased font size
        plt.suptitle(f'Inverted UpSet Plot for {regulation_type.capitalize()} Metabolites ({num_metabolites} metabolites)', 
                    fontsize=20)
        
        # Add legend
        legend_ax = plt.subplot(gs[2, 1])
        legend_ax.axis('off')
        legend_text = (f"This plot shows {num_metabolites} {regulation_type.lower()} metabolites and their presence across conditions.\n"
                       f"Each row represents a metabolite, and each column represents a condition.\n"
                       f"Dots indicate that a metabolite is present in that condition.")
        legend_ax.text(0.5, 0.5, legend_text, ha='center', va='center', fontsize=12)
        
        # Adjust layout
        plt.tight_layout()
        plt.subplots_adjust(top=0.95)
        
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        
        # Save the plot with appropriate DPI for readability
        print(f"Saving plot to {output_file}...")
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"Inverted UpSet plot saved to {output_file}")
        return True
        
    except Exception as e:
        print(f"Error creating inverted UpSet plot: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def create_upset_plots_for_Global_Overlap(regulation_type, base_path=None, condition_order=None, max_metabolites=None):
    """
    Create inverted UpSet plots for Type 2 data.
    
    Args:
        regulation_type (str): 'upregulated' or 'downregulated'
        base_path (str, optional): Base path to use instead of default
        condition_order (list, optional): Custom ordering of conditions
        max_metabolites (int, optional): Maximum number of metabolites to include in the plot
    """
    # Define file paths
    if not base_path:
        base_path = os.getcwd()  # Use current working directory as base path
    
    # Input and output paths
    input_path = os.path.join(base_path, "Infographics", regulation_type.lower())
    output_path = input_path  # Save outputs in the same location
    
    # CSV filename
    csv_filename = f"Global_Overlap_{regulation_type.lower()}_venn_data.csv"
    Global_Overlap_csv_path = os.path.join(input_path, csv_filename)
    
    # Check if the CSV file exists
    if not os.path.exists(Global_Overlap_csv_path):
        print(f"Error: Type 2 CSV file not found at {Global_Overlap_csv_path}")
        return
    
    # Output filename
    file_extension = "pdf"  # Use PDF for high-quality output
    upset_output_file = os.path.join(output_path, f"{regulation_type.lower()}_upset.{file_extension}")
    
    # Create the inverted UpSet plot
    create_inverted_upset_plot(
        Global_Overlap_csv_path, 
        upset_output_file, 
        regulation_type,
        custom_conditions=condition_order,
        max_metabolites=max_metabolites
    )

# R script for hierarchical clustering analysis
r_script = r'''
# Load required libraries
library(pvclust)


# Function to process metabolite data and create a presence/absence matrix
process_metabolite_data <- function(metabolite_data) {
  # Extract unique conditions
  all_conditions <- unique(unlist(strsplit(metabolite_data$Conditions, ",")))
  
  # Create an empty matrix
  matrix_data <- matrix(0, nrow = nrow(metabolite_data), ncol = length(all_conditions))
  rownames(matrix_data) <- metabolite_data$Metabolite
  colnames(matrix_data) <- all_conditions
  
  # Fill the matrix based on presence/absence
  for (i in 1:nrow(metabolite_data)) {
    conditions <- unlist(strsplit(metabolite_data$Conditions[i], ","))
    for (condition in conditions) {
      matrix_data[i, condition] <- 1
    }
  }
  
  return(matrix_data)
}


# Function to perform hierarchical clustering (only dendrogram)
perform_hierarchical_clustering <- function(data_matrix, output_file, regulation_type) {
  if (nrow(data_matrix) > 1) {
    # Check for rows with zero variance and remove them
    row_variance <- apply(data_matrix, 1, var)
    if (any(row_variance == 0)) {
      message("Removing rows with zero variance for ", regulation_type)
      data_matrix <- data_matrix[row_variance > 0, ]
    }
    
    # If no rows are left after filtering, skip clustering
    if (nrow(data_matrix) < 2) {
      message("Not enough metabolites with variance for clustering in ", regulation_type)
      return()
    }
    
    # Calculate distance matrix using correlation method
    dist_matrix <- as.dist(1 - cor(t(data_matrix), method = "pearson"))
    
    # Perform hierarchical clustering
    hc <- hclust(dist_matrix, method = "complete")
    
    # Bootstrap for p-values
    pv_result <- pvclust(data_matrix, 
                         method.dist = "correlation",
                         method.hclust = "complete",
                         nboot = 1000)
    
    # Save output
    pdf(output_file, width = 15, height = 10)
    
    # Plot dendrogram
    plot(pv_result, main = "") # Set main="" to remove the title
    pvrect(pv_result, alpha = 0.1)
    
    # Add title
    title(main = paste(regulation_type, "Metabolites Hierarchical Clustering"))
    
    dev.off()
  } else {
    message("Not enough metabolites for clustering in ", regulation_type)
  }
}

# Function to analyze venn data
analyze_metabolites_venn <- function(input_file, output_dir, regulation_type) {
  # Read the venn data
  venn_data <- read.csv(input_file)
  
  # Create output directory if it doesn't exist
  if (!dir.exists(output_dir)) {
    dir.create(output_dir, recursive = TRUE)
  }
  
  # Process data to create a presence/absence matrix
  presence_matrix <- process_metabolite_data(venn_data)
  
  # Output file path
  output_file <- file.path(output_dir, paste0(regulation_type, "_metabolite_clusters.pdf"))
  
  # Perform hierarchical clustering
  perform_hierarchical_clustering(presence_matrix, output_file, regulation_type)
  
  # Return processed data
  return(list(
    raw_data = venn_data,
    presence_matrix = presence_matrix
  ))
}

# Main execution
# Process upregulated metabolites
upregulated_results <- analyze_metabolites_venn(
  input_file = "./Infographics/upregulated/Global_Overlap_upregulated_venn_data.csv",
  output_dir = "./Infographics/upregulated",
  regulation_type = "Upregulated"
)

# Process downregulated metabolites
downregulated_results <- analyze_metabolites_venn(
  input_file = "./Infographics/downregulated/Global_Overlap_downregulated_venn_data.csv", 
  output_dir = "./Infographics/downregulated",
  regulation_type = "Downregulated"
)

# Print summary
cat("Analysis completed.\n")
cat("Upregulated metabolites analyzed:", nrow(upregulated_results$raw_data), "\n")
cat("Downregulated metabolites analyzed:", nrow(downregulated_results$raw_data), "\n")
'''

# Suppress specific warnings and logging
def configure_logging():
    # Suppress pandas FutureWarnings
    warnings.filterwarnings('ignore', category=FutureWarning)
    
    # Suppress R messages
    rpy2_logger.setLevel(logging.ERROR)
    
    # Configure logging format for essential messages only
    logging.basicConfig(
        level=logging.INFO,
        format='%(message)s'
    )

def print_directory_structure():
    directory_structure = """
Differential Producibility Analysis Pipeline Completed.!

Directory Structure is as follows -

base/
├── Expression/                   # Expression data files
├── JyMet2.3/                     # SurreyFBA executable package
├── Problems_FBA/                 # Problem files for Flux Balance Analysis (FBA)
├── Problems_KO/                  # Problem files for Knockout (KO) analysis
├── FBA_result/                   # FBA analysis outputs
├── KO_result/                    # KO analysis outputs
├── Producibility_Results/        # Producibility and mapping results
├── RankProd_Inputs/              # Rank Product inputs - upregulated and downregulated matrices
├── Results/                      # Rank Product Analysis results
├── Top_Ranks/                    # Top ranked metabolites
│   ├── Downregulated/
│   └── Upregulated/
└── Infographics/                 # Visualization outputs
    ├── Downregulated/
    └── Upregulated/

"""
    print(directory_structure)

# Define the DualOutput class
class DualOutput:
    def __init__(self, terminal, log_file):
        self.terminal = terminal
        self.log_file = log_file

    def write(self, message):
        self.terminal.write(message)
        self.log_file.write(message.encode('utf-8').decode('utf-8'))

    def flush(self):
        self.terminal.flush()
        self.log_file.flush()
    
    def input(self, prompt):
        self.write(prompt)
        return input(prompt)
    
######################################################################################################################################################################################
# Define the main function

def main():
    if len(sys.argv) > 1 and sys.argv[1] == "--help":
        display_help()
        sys.exit(0)
    
    # Track start time
    start_time = time()
    errors = []  # List to store errors encountered during execution

    # Initialize config and log_file to None
    config = None
    log_file = None

    try:
        # Initialize R and import rankprod package
        pandas2ri.activate()
        rankprod = importr('RankProd')

        base_dir = Path("./")  # Define base directory as a Path object

        # Step 1: Find the first .sfba file in the directory
        sfba_files = list(base_dir.glob("*.sfba"))
        if not sfba_files:
            raise FileNotFoundError("No .sfba files found in the directory.")
        model_path = sfba_files[0]  # Use the first matching file as Path object

        # Detect the operating system
        os_name = platform.system()

        # Set the fba_executable path based on the OS
        if os_name == "Windows":
            fba_executable = base_dir / "JyMet2.3" / "bin" / "Windows" / "sfba-glpk.exe"
        elif os_name == "Linux":
            fba_executable = base_dir / "JyMet2.3" / "bin" / "Linux" / "sfba-glpk"
        elif os_name == "Darwin":  # 'Darwin' is the kernel name for macOS
            fba_executable = base_dir / "JyMet2.3" / "bin" / "Mac" / "sfba-glpk"
        else:
            raise OSError(f"Unsupported operating system: {os_name}")

        # Step 2: Create configuration with all necessary paths
        config = Config(
            base_dir=base_dir,
            model_path=model_path,
            problems_dir_fba=base_dir / "Problems_FBA",
            problems_dir_ko=base_dir / "Problems_KO",
            template_fba="Problem_template_FBA.pfile",
            template_ko="Problem_template_KO.pfile",
            output_dir_fba=base_dir / "FBA_result",
            output_dir_ko=base_dir / "KO_result",
            fba_executable=fba_executable,  # Provide the fba_executable path
            expression_dir=base_dir / "Expression",
            output_dir=base_dir / "Results",
            producibility_results_dir=base_dir / "Producibility_Results",
            rankprod_output_dir=base_dir / "RankProd_Inputs",
            top_ranks_dir=base_dir / "Top_Ranks"
        )

        clean_directories_if_needed(config)
       
        # Define log file path for terminal output (AFTER config is created)
        terminal_log_path = Path(config.base_dir) / "DPA_Terminal_Output_Log.txt"
        
        # Open the log file for writing
        log_file = open(terminal_log_path, "w")
        
        # Write timestamp and execution start message to the log file
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_file.write(f"Timestamp: {timestamp}\n")
        log_file.write("DPA Tool Execution Started\n")
        log_file.write("=" * 50 + "\n\n")
        
        # Create a DualOutput object
        dual_output = DualOutput(sys.stdout, log_file)
        
        # Redirect stdout and stderr to the DualOutput object
        sys.stdout = dual_output
        sys.stderr = dual_output

        # Step 3: Get user preference for prioritization
        prioritization = 'higher'  # Default value
        user_input = dual_output.input("Enter prioritization method ('higher' or 'lower', default is 'higher'): ").lower()
        if user_input == 'lower':
            prioritization = 'lower'

        # Step 4: Get regulation threshold
        regulation_threshold = get_regulation_thresholds()

        # Step 5: Run pipeline analysis
        print("Starting pipeline analysis...")
        pipeline_analysis_main(config)

        # Step 6: Clean problem files with numeric-only names
        clean_problem_files(config.base_dir, ['Problems_FBA', 'Problems_KO'])

        # Step 7: Perform metabolite analysis
        print("\nStarting metabolite analysis...")
        analyzer = MetaboliteAnalyzer(
            config.base_dir, 
            config.producibility_results_dir, 
            prioritization,
            regulation_threshold
        )
        analyzer.analyze_metabolites()

        # Step 8: Generate RankProd matrices
        print("\nStarting RankProd matrix generation...")
        rankprod_generation_main(config)

        # Step 9: Perform RankProd analysis
        print("\nStarting RankProd analysis...")
        input_dir = "RankProd_Inputs"
        up_file = os.path.join(input_dir, "upregulated_matrix.csv")
        down_file = os.path.join(input_dir, "downregulated_matrix.csv")

        if not os.path.exists(up_file) or not os.path.exists(down_file):
            raise FileNotFoundError("Input files not found in RankProd_Inputs directory")

        up_matrix = read_and_prepare_matrix(up_file)
        down_matrix = read_and_prepare_matrix(down_file)

        conditions = get_condition_names(up_matrix)
        print("\nDetected conditions:", ", ".join(conditions))

        # Generate all permutations of conditions
        permutations = get_all_permutations(conditions)
        total_permutations = len(permutations)

        # Get user-selected permutations
        selected_permutations = get_user_selected_permutations(permutations)

        # Perform analysis only on the selected permutations
        perform_analysis_on_selected_permutations(up_matrix, down_matrix, selected_permutations)

        print(f"\nAnalysis completed successfully. Results have been saved to the Results directory.")
        
        # Step 10: Extract top-ranking metabolites
        extract_top_metabolites()

        # Step 11: Process visualizations and R analysis
        # Create Venn diagrams for both upregulated and downregulated
        regulation_types = ['Upregulated', 'Downregulated']
        
        for regulation_type in regulation_types:
            print(f"\n{'='*50}")
            print(f"Processing {regulation_type} metabolites...")
            print(f"{'='*50}")
            
            # Condition-specific Venn diagrams and CSV outputs
            create_condition_specific_venn_diagrams(regulation_type)
            
            # Global overlap Venn diagram and CSV output for all conditions
            create_global_overlap_venn_diagram(regulation_type)
        
        # Parse command-line arguments for UpSet plots
        parser = argparse.ArgumentParser(description='Create inverted UpSet plots for metabolite data')
        parser.add_argument('--base_path', type=str, help='Base path to data directory')
        parser.add_argument('--regulation_types', type=str, nargs='+', 
                            default=['Upregulated', 'Downregulated'],
                            help='Regulation types to process (default: Upregulated Downregulated)')
        parser.add_argument('--condition_order', type=str, nargs='+',
                            help='Custom ordering of conditions in the plot')
        parser.add_argument('--max_metabolites', type=int,
                            help='Maximum number of metabolites to include in the plot')
        
        args = parser.parse_args()
        
        regulation_types = args.regulation_types
        base_path = args.base_path
        
        for regulation_type in regulation_types:
            print(f"\n{'='*50}")
            print(f"Processing {regulation_type} metabolites...")
            print(f"{'='*50}")
            create_upset_plots_for_Global_Overlap(
                regulation_type,
                base_path=base_path,
                condition_order=args.condition_order,
                max_metabolites=args.max_metabolites
            )
        
        # Step 12: Run R hierarchical clustering analysis
        robjects.r(r_script)
        
        configure_logging()
        
        print_directory_structure()

    except Exception as e:
        # Capture any errors during execution
        errors.append(str(e))
        print(f"Error during execution: {e}")

    finally:
        # Calculate execution time
        execution_time = time() - start_time

        # Write execution time to the log file
        if log_file is not None:
            log_file.write(f"\nExecution Time: {execution_time:.2f} seconds\n")

        # Restore stdout and stderr
        sys.stdout = sys.__stdout__
        sys.stderr = sys.__stderr__
        
        # Close the log file if it was opened
        if log_file is not None:
            log_file.close()

if __name__ == "__main__":
    main()